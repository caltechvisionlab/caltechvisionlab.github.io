---
title: "Perona Lab - Home"
layout: homelay
excerpt: "Perona Lab"
sitemap: false
permalink: /mronchi/
---

<!DOCTYPE html>
<html lang="en">
    
    <head>
        <!-- Page Meta -->
        <meta charset="utf-8">
        <meta http-equiv="Content-Type" content="text/html">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <!-- Content Meta -->
        <title>Homepage of Matteo Ruggero Ronchi</title>
        <meta name="description" content="Matteo Ruggero Ronchi - Personal Web page">
        <meta name="author" content="Matteo Ruggero Ronchi">
        <meta name="keywords" content="Matteo Ruggero Ronchi, Caltech, Computational Vision Lab, Pietro Perona">
        <!-- Icon -->
        <link rel="icon" href="./assets/icon/caltech_transparent.png">
            
        <!-- Bootstrap core CSS -->
        <link href="./assets/css/bootstrap.css" rel="stylesheet">
        <!-- Custom styles for this template -->
        <link href="./assets/theme/navbar.css" rel="stylesheet">
                                        
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->

        <!--<style>
            li.distance_est {
                background: url('./assets/icon/dist_est.png') no-repeat left top;
                height: 54px;
                padding-left: 44px;
                padding-top: 3px;
            }
            li.cocoa {
                src: './assets/icon/cocoa.png' no-repeat left top;
                height: 54px;
                padding-left: 44px;
                padding-top: 3px;
            }
        </style>-->
       <script type="text/javascript">
         function confirm_alert(node) {
           return confirm("Coming Soon!");
         }
       </script>
       <script>
         (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

         ga('create', 'UA-54004953-3', 'auto');
         ga('send', 'pageview');
       </script>
       <script>
       /**
         * Function that tracks a click on an outbound link in Analytics.
         * This function takes a valid URL string as an argument, and uses that URL string
         * as the event label. Setting the transport method to 'beacon' lets the hit be sent
         * using 'navigator.sendBeacon' in browser that support it.
         */
         var trackOutboundLink = function(url) {
         ga('send', 'event', 'outbound', 'click', url, {
         'transport': 'beacon',
         'hitCallback': function(){document.location = url;}
         });
         }
       </script>

    </head>
    
    <body>
        <div class="container">
            
            <div class="page-header">
                <h1>
                    <b>Matteo Ruggero Ronchi</b>
                    <small>PhD & Postdoc @ CALTECH</small>
                </h1>
            </div>
            
            <!-- Static navbar -->
            <div class="navbar navbar-default" role="navigation">
                <div class="container-fluid">
                    
                    <!-- Static navbar HEADER -->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand" href="http://www.caltech.edu"><img src="./assets/icon/caltech_transparent.png" height="50px" /></a>
                        <a class="navbar-brand" href="http://www.vision.caltech.edu"><img src="./assets/icon/cv_lab_logo.png" height="48px" /></a>
		    </div>
                    
                    <!-- Static navbar BODY -->
                    <div class="navbar-collapse collapse">
                        <!-- LEFT -->
                        <ul class="nav navbar-nav">
                            
                            <li><a href="#bio">Bio</a></li>
                            <!-- <li class="active"><a href="intro">Intro</a></li> -->
                            <li><a href="#research">Research Projects</a></li>
                            <!-- <li><a href="#publications">Publications</a></li> -->
                            <li><a href="#cv">CV</a></li>
                            <li><a href="#extras">Extra</a></li>

                        </ul>
                        <!-- RIGHT -->
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="https://mrronchi.github.io/" onclick="return confirm_alert(this);">Personal Web Page</a></li>
                            <li><a href="#contact">Contact</a></li>
                            <!--
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Dropdown <span class="caret"></span></a>
                                <ul class="dropdown-menu" role="menu">
                                    <li><a href="#">Action</a></li>
                                    <li><a href="#">Another action</a></li>
                                    <li><a href="#">Something else here</a></li>
                                    <li class="divider"></li>
                                    <li class="dropdown-header">Nav header</li>
                                    <li><a href="#">Separated link</a></li>
                                    <li><a href="#">One more separated link</a></li>
                                </ul>
                            </li>-->
                        </ul>
                    </div><!--/.nav-collapse -->
                    
                </div><!--/.container-fluid -->
            </div><!--/.Static navbar -->
            
            <!-- Main component for a primary marketing message or call to action
            <div class="jumbotron">
                <h1>Navbar example</h1>
                <p>This example is a quick exercise to illustrate how the default, static navbar and fixed to top navbar work. It includes the responsive CSS and HTML, so it also adapts to your viewport and device.</p>
                <p>
                <a class="btn btn-lg btn-primary" href="" role="button">View navbar docs &raquo;</a>
                </p>
            </div> -->
            
            <div class="section">
            <section id="bio"> </section>
            <h2 style="margin-top:50px"><b>Brief Bio</b></h2>
            <p style="margin-bottom:20px"></p>
            <div class="cf">
                <!-- <div> <img src="./data/portfolio/RIMG0712.JPG" height="250px" /> </div> -->
                <div> <img src='./data/portfolio/RIMG0712.JPG' height="250px" onmouseover="this.src='./data/portfolio/me_2017.jpg';" onmouseout="this.src='./data/portfolio/RIMG0712.JPG';" /> </div>
	        <div class="fill"> <p> </div>
                <div> 
<p align="justify">
My research interests lie in the areas of Computer Vision and Robotics. In particular, I am interested in the task of estimating the pose (2D and 3D) of humans portrayed in pictures and studying how can that (i) be done if there is small availability of supervised training data, and (ii) be applied to the task of controlling the motor skills of robots.</p>

<p align="justify">I completed my PhD in Computer Science (2013-2019) and Postdoctoral studies in the <a href="http://www.vision.caltech.edu/">Computational Vision Lab at Caltech</a>, having enjoyed the great fortune of being advised by <a href="http://www.vision.caltech.edu/Perona.html">Pietro Perona</a>.</p> 

<p align="justify">Prior to that, I attended the <a href="http://www.diism.unisi.it/en">University of Siena</a>, in Italy, where I received both my Bachelor (2007-2010) and Master's (2010-2012) Degree in Computer Science and Information Engineering. During that period, I was fortunate to be supervised by Michelangelo Diligenti and Marco Gori and worked on the application of machine learning methods to the problems of <a href="https://en.wikipedia.org/wiki/Steganalysis">Image Steganalysis</a> and unsupervised <a href="https://en.wikipedia.org/wiki/Feature_learning">Feature Learning</a> for visual recognition.</p>
               
<a href="https://sites.google.com/site/matteoruggeror/">Old Website</a>


            </div>
            </div>
            </div>
            <hr>
           
            <div class="section">
            <section id="research"> </section>
            <h2 style="margin-top:50px"><b>Research Projects</b></h2>
            <p style="margin-bottom:20px"></p>

            <div class="cf" style="margin-bottom:15px">
                <div> <img src="./assets/icon/coming-soon.png" width="150px" height="150px" style="border:1px solid #0000FF"/> </div>
                <div style="padding-left:10px;">
                    <div class="cf">
                    <!--<div><img src="./assets/icon/NEW-icon.png" width="50px"/></div>-->
                    <div>
                       <p style="margin-top:12px;"><b>How Much does Multi-View Self-Supervision Help 3D Pose Estimation?</b></p>
                    </div>
                    </div>
                    <p align="justify"> 3D pose estimation from a single image is challenging due to both the inherent ambiguity of the task and the difficulty of collecting large and varied supervised training datasets. Self-supervised learning has emerged as an alternative solution where the aim is to learn features that encode pose information without requiring explicit supervision. This feature learning step is typically framed as solving a pretext task that can capture information about human pose, such as temporal alignment of video frames or multi-view reconstruction. However, directly comparing current approaches is difficult due to different datasets and experimental settings used. In this paper we standardize this comparison by performing a detailed evaluation of multi-view self-supervised feature learning methods. Through experiments on Human3.6M, we observe several important issues that arise when using multi-view information as a training signal, including the impact of the amount of supervised pose data, the 3D reference frame used, the power of different pose decoders, among others. We conclude with recommendations and by highlighting open questions.
                    </p>
                    <p><a href="http://www.vision.caltech.edu/~mronchi/projects/MultiView3DPose/" onclick="return confirm_alert(this);"><span class="label_download">Project Website</span></a></p>
                </div>
            </div>
            <hr>

            <div class="cf" style="margin-bottom:15px">
                <div> <img src="./papers/BMVC18_Relative3DPose_FIRST.png" width="150px" height="210px" style="border:1px solid #0000FF"/> </div>
                <div style="padding-left:10px;">
                    <div class="cf">
                    <div>
                       <p style="margin-top:12px;"><b>It's all Relative: Monocular 3D Human Pose Estimation from Weakly Supervised Data (BMVC'18)</b></p>
                    </div>
                    </div>
                    <p align="justify">We address the problem of 3D human pose estimation from 2D input images using only weakly supervised training data. Despite showing considerable success for 2D pose estimation, the application of supervised machine learning to 3D pose estimation in real world images is currently hampered by the lack of varied training images with associated 3D poses. Existing 3D pose estimation algorithms train on data that has either been collected in carefully controlled studio settings or has been generated synthetically. Instead, we take a different approach, and propose a 3D human pose estimation algorithm that only requires relative estimates of depth at training time. Such training signal, although noisy, can be easily collected from crowd annotators, and is of sufficient quality for enabling successful training and evaluation of 3D pose algorithms. Our results are competitive with fully supervised regression based approaches on the Human3.6M dataset, despite using significantly weaker training data. Our proposed approach opens the door to using existing widespread 2D datasets for 3D pose estimation by allowing fine-tuning with noisy relative constraints, resulting in more accurate 3D poses.</p> 
                    <p><a href="http://www.vision.caltech.edu/~mronchi/projects/RelativePose/" onclick="trackOutboundLink('http://www.vision.caltech.edu/~mronchi/projects/RelativePose/'); return false;"><span class="label_download">Project Website</span></a></p>
                </div>
            </div>
            <hr>


            <div class="cf" style="margin-bottom:15px">
                <div> <img src="./papers/ICCV17_PoseErrorDiagnosis_FIRST.png" width="150px" height="210px" style="border:1px solid #0000FF"/> </div>
                <div style="padding-left:10px;">
                    <div class="cf">
                    <div><!--<img src="./assets/icon/NEW-icon.png" width="50px"/>--></div>
                    <div>
                       <p style="margin-top:12px;"><b>Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation (ICCV'17)</b></p>
                    </div>
                    </div>
                    <p align="justify"> We propose a new method to analyze the impact of errors in algorithms for multi-instance pose estimation and a principled benchmark that can be used to compare them. We define and characterize three main classes of errors - localization, scoring, and background - study how they are influenced by instance attributes and their impact on an algorithm's performance. Our technique is applied to compare the two leading methods for human pose estimation on the COCO Dataset, measure the sensitivity of pose estimation with respect to instance size, type and number of visible keypoints, clutter due to multiple instances, and the relative score of instances. The performance of algorithms, and the types of error they make, are highly dependent on all these variables, but mostly on the number of keypoints and the clutter. The analysis and software tools we propose offer a novel and insightful approach for understanding the behavior of pose estimation algorithms and an effective method for measuring their strengths and weaknesses. </p>
                    <p><a href="http://www.vision.caltech.edu/~mronchi/projects/PoseErrorDiagnosis/" onclick="trackOutboundLink('http://www.vision.caltech.edu/~mronchi/projects/PoseErrorDiagnosis/'); return false;"><span class="label_download">Project Website</span></a></p>
                </div>
            </div>

<hr>

            <div class="cf" style="margin-bottom:15px">
                <div> <img src="./papers/ICDM16_RotationInvariantMovemeDiscovery_FIRST.png" width="150px" height="210px" style="border:1px solid #0000FF"/> </div>
                <div style="padding-left:10px;">
                    <div class="cf">
                    <div><!--<img src="./assets/icon/NEW-icon.png" width="50px"/>--></div>
                    <div>
                       <p style="margin-top:12px;"><b>A Rotation Invariant Latent Factor Model for Moveme Discovery from Static Poses (ICDM'16)</b></p> 
                    </div>
                    </div>
                    <p align="justify"> We tackle the problem of learning a rotation invariant latent factor model when the training data is comprised of lower-dimensional projections of the original feature space. The main goal is the discovery of a set of 3-D bases poses that can characterize the manifold of primitive human motions, or movemes, from a training set of 2-D projected poses obtained from still images taken at various camera angles. The proposed technique for basis discovery is data-driven rather than hand-designed. The learned representation is rotation invariant, and can reconstruct any training instance from multiple viewing angles. We apply our method to modeling human poses in sports (via the Leeds Sports Dataset), and demonstrate the effectiveness of the learned bases in a range of applications such as activity classification, inference of dynamics from a single frame, and synthetic representation of movements. </p>
                    <p><a href="http://www.vision.caltech.edu/~mronchi/projects/RotationInvariantMovemes/" onclick="trackOutboundLink('http://www.vision.caltech.edu/~mronchi/projects/RotationInvariantMovemes/'); return false;"><span class="label_download">Project Website</span></a></p>
                </div>
            </div>

<hr>

            <div class="cf" style="margin-bottom:15px">
                <div> <img src="./papers/BMVC15_DescribingCommonVisualActions_FIRST.png" width="150px" height="210px" style="border:1px solid #0000FF"/> </div>
                <div style="padding-left:10px;">
                    <p><b>Describing Common Human Visual Actions in Images (BMVC'15)</b></p>
                    <p align="justify">
Which common human actions and interactions are recognizable in monocular still images? Which involve objects and/or other people? How many is a person performing at a time? We address these questions by exploring the actions and interactions that are detectable in the images of the MS COCO dataset. We make two main contributions. First, a list of 140 common ‘visual actions’, obtained by analyzing the largest on-line verb lexicon currently available for English (VerbNet) and human sentences used to describe images in MS COCO. Second, a complete set of annotations for those ‘visual actions’, composed of subject-object and associated verb, which we call COCO-a (a for ‘actions’). COCO-a is unique because it is data-driven, rather than experimenter-biased, and all subjects and objects are localized. A statistical analysis of the accuracy of our annotations and of each action, interaction and subject-object combination is provided.
                    </p>
                    <p><a href="http://www.vision.caltech.edu/~mronchi/projects/Cocoa/" onclick="trackOutboundLink('http://www.vision.caltech.edu/~mronchi/projects/Cocoa/'); return false;"><span class="label_download">Project Website</span></a></p>
                </div>
            </div>

<hr>

            <div class="cf" style="margin-bottom:15px">
                <div> <img src="./papers/ECCV14_FaceDistancePortrait_FIRST.png" width="150px" height="210px" style="border:1px solid #0000FF"/> </div>
                <div style="padding-left:10px;">
                    <p><b>Distance Estimation of an Unknown Person from a Portrait (ECCV'14)</b></p>
                    <p align="justify">
We propose the first automated method for estimating distance from frontal pictures of unknown faces. Camera calibration is not necessary, nor is the reconstruction of a 3D representation of the shape of the head. Our method is based on estimating automatically the position of face and head landmarks in the image, and then using a regressor to estimate distance from such measurements. We collected and annotated a dataset of frontal portraits of 53 individuals spanning a number of attributes (sex, age, race, hair), each photographed from seven distances. We find that our proposed method outperforms humans performing the same task. We observe that different physiognomies will bias systematically the estimate of distance, i.e. some people look closer than others. We explore which landmarks are more important for this task.
                    </p>
                    <p><a href="http://www.vision.caltech.edu/~mronchi/projects/FaceDistancePortrait/" onclick="trackOutboundLink('http://www.vision.caltech.edu/~mronchi/projects/FaceDistancePortrait/'); return false;"><span class="label_download">Project Website</span></a></p>
                </div>
            </div>
            </div>

            <hr>
            <div class="section">
            <section id="cv"> </section>
            <h2 style="margin-top:50px"><b>Curriculum Vitae</b></h2>
            <p style="margin-bottom:20px"></p>
                <a class="btn btn-large btn-primary" type="button" href="data/CV/CV_MRRonchi_04_2020.pdf" target="_blank" onclick="_gaq.push(['_trackEvent','Download','data',this.href])">
                    <b>Download CV (April 2020)</b>
                </a>
            </div>
 
            <!-- <div class="section">
            <section id="downloads"> </section>
            <h2 style=""><b>Downloads</b></h2>
            <ul><li>Under Construction</li></ul>
            </div>-->
            <hr> 
            <div class="section">
            <section id="extras"> </section>
            <h2 style="margin-top:50px"><b>Extra</b></h2>
              <p style="margin-bottom:20px"></p>
              <iframe width="420" height="315" src="https://www.youtube.com/embed/3Bsp7d3nt5Y?start=1285" allowfullscreen></iframe>
              <iframe width="420" height="315" src="http://www.youtube.com/embed/pVWqA1LhFIA?autoplay=0" allowfullscreen></iframe>
              <!-- allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" -->
            </div>
               
            <hr>        
            <div class="section">
            <section id="contact"> </section>
            <h2 style="margin-top:50px"><b>Contact</b></h2>
                
                <p>© 2020,
                    <a href="http://vision.caltech.edu/%7Emronchi/">Matteo Ruggero Ronchi</a>
                </p>
                
                <form class="email" role="form" width="100px" id="email-form">
                    <div class="form-group">
                        <label for="name">Your message:</label>
                        <textarea class="form-control" rows="15" id="email-text">Write your email here.</textarea>
                    </div>
                    <div class="form-group">
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(1); return false">Send with email client</button>
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(2); return false">Send with Gmail</button>
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(3); return false">Send with Yahoomail</button>
                    </div>
                </form>
                
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
    
        </div> <!-- /container -->
        
        <!-- Bootstrap core JavaScript
         ================================================== -->
        <!-- Placed at the end of the document so the pages load faster -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
        <script src="./assets/js/bootstrap.min.js"></script>
        <!-- sendEmail java script source file -->
        <script src="./assets/js/sendEmail.js"></script>
        <script src="./assets/js/sendGmail.js"></script>

    </body>
</html>
