---
title: "Perona Lab - Home"
layout: null
excerpt: "Perona Lab"
sitemap: false
permalink: /mronchi/projects/Cocoa
---

<!DOCTYPE html>
<html lang="en">
    
    <head>
        <link rel="stylesheet" href="https://code.jquery.com/ui/1.11.1/themes/smoothness/jquery-ui.css" />

        <!-- Google analytics JavaScript
         ================================================== -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-54004953-2', 'auto');
          ga('send', 'pageview');
        </script>
       
        <!-- JQuery and JQuery UI
        ================================================= -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
        <script src="http://code.jquery.com/jquery-1.11.1.min.js"></script>
        <script src="http://code.jquery.com/ui/1.11.1/jquery-ui.min.js"></script>
        
        <!-- Page Meta -->
        <meta charset="utf-8">
        <meta http-equiv="Content-Type" content="text/html">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <!-- Content Meta -->
        <title>Describing Common Human Visual Actions in Images</title>
        <meta name="description" content="Describing Common Human Visual Actions in Images - Project Page">
        <meta name="author" content="Matteo Ruggero Ronchi">
        <meta name="keywords" content="Matteo Ruggero Ronchi, Pietro Perona, Action recognition, Human actions, Visual Actions, Visual Verbnet, MS COCO, COCO-a, caltech">
        <!-- Icon -->
        <link rel="icon" href="/archive/ronchi_matteo/assets/icon/caltech_transparent.png">
            
        <!-- Bootstrap core CSS -->
        <link href="/archive/ronchi_matteo/assets/css/bootstrap.css" rel="stylesheet">
        <!-- Custom styles for this template -->
        <link href="/archive/ronchi_matteo/assets/theme/navbar.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
        
    </head>
    
    <body>
        <div class="container">
            
            <div class="page-header">
                <h1>
                    <b>Describing Common Human Visual Actions in Images</b>
                    <small>BMVC'15</small>
                </h1>
            </div>
            
            <!-- Static navbar -->
            <div class="navbar navbar-default" role="navigation">
                <div class="container-fluid">
                    
                    <!-- Static navbar HEADER -->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand" href="http://www.caltech.edu"><img src="/archive/ronchi_matteo/assets/icon/caltech_transparent.png" height="50px" /></a>
                    </div>
                    
                    <!-- Static navbar BODY -->
                    <div class="navbar-collapse collapse">
                        <!-- LEFT -->
                        <ul class="nav navbar-nav">

                            <li><a href="#intro">Intro</a></li>

                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Paper<span class="caret"></span></a>
                                <ul class="dropdown-menu">
                                  <li><a href="#contributions">Contributions</a></li>
                                  <li><a href="#framework">Framework</a></li>
                                  <li><a href="#dataset">Coco-a Dataset</a></li>
                                  <li><a href="#analysis">Analysis</a></li>
                                  <!--<li role="separator" class="divider"></li>-->
                                </ul>
                            </li>
                           
		            <li><a href="http://aws-ec2-cit-mronchi.org/explore">Explore</a></li> 
                            <li><a href="#download">Download</a></li>
                            <li><a href="#cite">Cite</a></li>

                        </ul>
                        <!-- RIGHT -->
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="#contact">Contact</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                    
                </div><!--/.container-fluid -->
            </div><!--/.Static navbar -->
            
            <div class="section">
            <section id="intro"> </section>
            <h2 style=""><b>Introduction</b></h2>
  
                <p align="justify">The idea that actions are an important component of scene understanding in computer vision dates back at least to the '80s. In order to detect actions alongside objects the relationships between those objects needs to be discovered. For each action the roles of `subject' (active agent) and `object' (passive - whether thing or person) have to be identified. This information may be expressed as a `semantic network', which is the first useful output of a vision system for scene understanding.
                </p> 
                <p align="justify">Three main challenges face us in approaching scene understanding:
                <ol> 
                    <li>Deciding the nature of the representation that needs to be produced (e.g. there is still disagreement on whether actions should be viewed as arcs or nodes in the semantic network).</li>
                    <li>Designing algorithms that will analyze the image and produce the desired representation.</li> 
                    <li>Learning -- most of the algorithms that are involved have a considerable number of free parameters.</li> 
                </ol>
                </p>
                <a href="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-intro.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image">
                    <img src="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-intro.png" >
                </a>
                <p align="justify">The ideal dataset to guide our next steps has four desiderata: 
                <ol>
                    <li>It is <i>representative</i> of the pictures we collect every day.</li>
                    <li>It is <i>richly and accurately annotated</i> with the type of information we would like our systems to know about.</li>
                    <li>It is <i>not biased</i> by a particular approach to scene understanding, rather it is collected and annotated independently of any specific computational approach.</li>
                    <li>It is <i>large</i>, containing sufficient data to train the large numbers of parameters that are present in today's algorithms.</li>
                </ol>
                Current datasets do not measure up to one or more of these criteria. Our goal is to fill this gap.
                </p>
                <p align="justify">In the present study we focus on actions that may be detected from single images (rather than video). We explore the visual actions that are present in the recently collected MS COCO image dataset. The MS COCO dataset is large, finely annotated and focussed on 81 commonly occurring objects and their typical surroundings. 
                </p>
 
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="contributions"> </section>
            <h2 style=""><b>Contributions</b></h2>
            
                <ul>
                    <li>
                        <p align="justify">An <b>unbiased method for estimating actions</b>, where the data tells us which actions occur, rather than starting from an arbitrary list of actions and collecting images that represent them. We are thus able to explore the type, number and frequency of the actions that occur in common images. The outcome of this analysis is <a href="#dataset"><b>Visual VerbNet (VVN)</b></a>, listing the 140 common actions that are visually detectable in images.</p>
                    </li>
                
                    <li>
                        <p align="justify">A <b>large and well annotated dataset of actions</b> on the current best image dataset for visual recognition, with rich annotations including all the actions performed by each person in the dataset, and the people and objects that are involved in each action, subject's posture and emotion, and high level visual cues such as mutual position and distance. The dataset is publicly available, see the <a href="#download">Download</a> section.
                        </p>
                    </li>
                </ul>
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="framework"> </section>
            <h2 style=""><b>Framework</b></h2>
                <p align="justify">We call <b><i>visual action</i></b> an action, state or occurrence that has a unique and unambiguous visual connotation, making it detectable and classifiable; i.e., <i>lay down</i> is a visual action, while <i>relax</i> is not. A visual action may be discriminable only from video data, <i>multi-frame visual action</i> such as <i>open</i> and <i>close</i>, or from monocular still images, <i>single-frame visual action</i> (simply <i>visual action</i>), such as <i>stand</i>, <i>eat</i> and <i>play tennis</i>. In order to label visual actions we will use the verbs that come readily to mind to a native English speaker, a concept akin to entry-level categorization for objects. Based on this criterion sometimes we prefer more general visual actions (e.g. <i>play tennis</i>) rather than the sports domain specific ones such as <i>volley</i> or <i>serve</i>, and <i>drink</i> rather than more specific motions such as <i>lift a glass to the lips</i>, other times more specific ones (e.g. <i>shaking hands</i> instead of more generally <i>greet</i>).
                </p>
                <p align="justify">While taxonomization has been adopted as an adequate means of organizing object categories (e.g. animal &rarr; mammal &rarr; dog &rarr; dalmatian), and shallow taxonomies are indeed available for verbs in <a href="http://verbs.colorado.edu/~mpalmer/projects/verbnet.html">VerbNet</a>, we are not interested in fine-grained categorization for the time being and do not believe that <a href="http://mscoco.org/">MS COCO</a> would support it either. Thus, there are no taxonomies in our set of visual actions.
                </p>
               
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="dataset"> </section>
            <h2 style=""><b>Coco-a Dataset</b></h2>
                
                <p align="justify">Our goal is to collect an unbiased dataset with a large amount of meaningful and detectable interactions involving human agents as subjects. We put together a process, exemplified in the Figure below, consisting of four steps.
                </p>

                <a href="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-dataset.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image">
                    <img src="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-dataset.png" >
                </a>

                <h4 style=""><b>1) Visual VerbNet</b></h4>
                <p align="justify">We obtained the list of common visual actions that are observed in everyday images, by a combined analysis of VerbNet and MS COCO captions. Our list, which we call Visual VerbNet attempts to include all actions that are visually discriminable. It avoids verb synonyms, actions that are specific to particular domains, and fine-grained actions. Unlike previous work, Visual VerbNet is not the result of experimenter’s idiosyncratic choices; rather, it is derived from linguistic analysis (<a href="http://verbs.colorado.edu/~mpalmer/projects/verbnet.html">VerbNet</a>) and an existing large dataset of descriptions of everyday scenes (<a href="http://mscoco.org/dataset/#download">MS COCO Captions</a>).
                </p>

                <h4 style=""><b>2) Image and Subject Selection</b></h4>
                <p align="justify">Different actions usually occur in different environments, so in order to balance the content of our dataset we selected an approximately equal number images of three types of scenes: sport, outdoor and indoor. We also selected images of various complexity, containing single subjects, small groups (2-4 subjects) and crowds (&gt; 4 subjects). Next, we identified who is carrying out actions (the subjects), as all the people in an image whose pixel area is larger than 1600 pixels. All the people, regardless of size, are still considered as possible objects of an interaction.
                </p> 

                <h4 style=""><b>3) Interactions Annotation</b></h4>
                <p align="justify">For each subject we identified the objects that he/she is interacting with, based on the agreement of 3 out of 5 Amazon Mechanical Turk annotators asked to evaluate each image.
                </p>

                <h4 style=""><b>4) Visual Actions Annotation</b></h4>
                <p align="justify">For each subject-object pair (and each single agent), we labelled all the possible actions and interactions involving that pair, along with high level visual cues such as emotion and posture of the subject, and spatial relationship and distance with the object of interaction (when present). We asked 3 annotators to select all the visual actions and adverbs that describe each subjet-object interaction pair. In some cases annotators interpreted interactions differently, but still correctly, so we return all the visual actions collected for each interaction along with the value of agreement of the annotators, rather than forcing a deterministic, but arbitrary, ground truth.
                </p>
    
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>

            <div class="section">
            <section id="analysis"> </section>
            <h2 style=""><b>Analysis</b></h2>

                <p align="justify">In the figure below we can see the most frequent types of actions carried out when subjects interact with four specific object categories: other people, animals, inanimate objects (such as a handbag or a chair) and food. For interactions with people the visual actions belong mostly to the category ‘social’ and ‘perception’. When subjects interact with animals the visual actions are similar to those with people, except there are fewer ‘social’ actions and more ‘perception’ actions. Person and animal are the only types of objects for which the ‘communication’ visual actions are used at all. When people interact with objects the visual actions used to describe those interactions are mainly from the categories ‘with objects’ and ‘perception’. As expected, food items are the only ones that have ‘nutrition’ visual actions.
                </p>
                <a href="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-visual-action-type-per-object.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image">
                    <img src="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-visual-action-type-per-object.png" >
                </a>

                <p align="justify">We show the 29 objects that people interact with (left) and the 31 visual actions that people perform (right) in the COCO-a dataset, having more than 100 occurrences. The human-centric nature of our dataset is confirmed by the fact that the most frequent object of interaction is other persons, an order of magnitude more than the other objects. Since our dataset contains an equal number of sports, outdoor and indoor scenes, the list of objects is heterogeneous and contains objects that can be found in all environments. It appears that the visual actions list has a very long tail, which leads to the observation that MS COCO dataset is sufficient for a thorough representation and study of about 20 to 30 visual actions. The most frequent visual action in our dataset is ‘be with’. This is a very particular visual action as annotators use it to specify when people belong to the same group. Common images often contain multiple people involved in different group actions, and this annotation can provide insights in learning concepts such as the difference between proximity and interaction – i.e. two people back to back are probably not part of the same group although spatially close.
                </p>

                <a href="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-frequencies.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image">
                    <img src="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-frequencies.png" >
                </a>

                <p align="justify">The COCO-a dataset contains a rich set of annotations. We provide two examples of the information that can be extracted and explored, for an object and a visual action contained
in the dataset. The figure below on the left describes interactions between people. We list the most frequent visual actions that people perform together, postures that are held, distances of interaction and locations. A similar analysis can be carried out for the visual action touch, (on the right).
                </p>

                <a href="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-detailed-stats.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image">
                    <img src="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-detailed-stats.png" >
                </a>

                <p align="justify">To explore the expressive power of our annotations we decided to query rare types of interactions and visualize the images retrieved. The figure below shows the result of querying our dataset for visual actions with rare emotion, posture, position or location combinations. The format of the annotations allows to query for images by specifying at the same time multiple properties of the interactions and their combinations, making them particularly suited for the training of image retrieval systems.
                </p>

                <a href="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-rare-queries.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image">
                    <img src="/archive/ronchi_matteo/projects/Cocoa/Figures/cocoa-rare-queries.png" >
                </a>

                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>

            <div class="section">
            <section id="download"> </section>
            <h2 style=""><b>Download - (Beta Version Available!)</b></h2>
            <p><a style="cursor:pointer;" onclick="showDialog()"><u><b>Click here to request the credentials</b></u></a> for the latest Coco-a version, and to receive all the updates on the status of the Dataset.</p>
            <!--<p><a style="cursor:pointer;" onclick="alert('Server currently down, send me an email!')"><u><b>Click here to request the credentials</b></u></a> for the latest Coco-a version, and to receive all the updates on the status of the Dataset.</p>-->

                <div id="request_pass_dialog" title="Request Credentials" style="display:none;">
                  <fieldset class="ui-helper-reset">
                    <div>Email:</div>
                    <input type="text" id="email_input" class="ui-widget-content ui-corner-all">
                  </fieldset>
                  <a class="btn btn-primary" type="button" id="request_button" style="background:#C2C1C1;margin-top:20px;" onclick="requestPass(this);"><b>Register</b></a>
                </div>

                <div class="row text-center">
                  <a class="btn btn-large btn-primary" type="button" onclick="" disabled>
                      <b>Coco-a Api</b>
                  </a>
                  <a class="btn btn-large btn-primary" href="https://data.caltech.edu/tindfiles/serve/11c868c9-1e03-4ee4-a50a-63168c892594/" type="button" onclick="ga('send', 'event', 'Dataset', 'Download', 'VVN', 6)">
                      <b>Visual VerbNet</b>
                  </a>
                  <a class="btn btn-large btn-primary" href="https://data.caltech.edu/tindfiles/serve/f391ebe5-f4b5-4ad0-a9ed-475d4e2a25b7/" type="button" onclick="ga('send', 'event', 'Dataset', 'Download', 'Coco-a Annotations', 7)">
                      <b>Coco-a Dataset</b>
                  </a>
                </div>

                <iframe id="my_iframe" style="display:none;"></iframe>
 
                Notes:
                <ul>
                    <li>Api coming soon, <a href='https://github.com/matteorr/coco-a'><b>click here for a demo</b></a>.</li>
                    <li>Visual VerbNet Version [1.0].</li>
                    <li>Coco-a Dataset Version [0.9-beta].</li>
                    <li><a href="http://mscoco.org/dataset/#download">MS COCO api and annotations</a> are required external library and data.</li>
                    <li> This code and data is licensed under the <a href="http://en.wikipedia.org/wiki/BSD_licenses" target="blank">Simplified BSD License</a>.</li>
                </ul>
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="cite"> </section>
            <h2 style=""><b>Cite</b></h2>
                <p>If you find our paper or the released data or code useful to your work, please cite:</p>
                <pre>@inproceedings{</br>  BMVC2015_52,</br>  title={Describing Common Human Visual Actions in Images},</br>  author={Matteo Ruggero Ronchi and Pietro Perona},</br>  year={2015},</br>  month={September},</br>  pages={52.1-52.12},</br>  articleno={52},</br>  numpages={12},</br>  booktitle={Proceedings of the British Machine Vision Conference (BMVC)},</br>  publisher={BMVA Press},</br>  editor={Xianghua Xie, Mark W. Jones, and Gary K. L. Tam},</br>  doi={10.5244/C.29.52},</br>  isbn={1-901725-53-7},</br>  url={https://dx.doi.org/10.5244/C.29.52}</br>}</pre> 
              <ul>
                <li>
                  Describing Common Human Visual Actions in Images <br>
                  M.R. Ronchi and P. Perona <br>
                  BMVC 2015, Swansea, Wales, September 2015. <br>
                      
                  <a href="/archive/ronchi_matteo/projects/Cocoa/Cite/BMVC15_DescribingCommonVisualActions_BIBTEX" target="_blank" onclick="ga('send', 'event', 'Cite', 'Download', 'Bibtex', 0)">
                  <span class="label_download">Bibtex</span>
                  </a> 

                  <a href="/archive/ronchi_matteo/papers/BMVC15_DescribingCommonVisualActions_ABSTRACT.pdf" target="_blank" onclick="ga('send', 'event', 'Cite', 'Download', 'Abstract', 1)">
                  <span class="label_download">Abstract</span>
                  </a>

                  <a href="/archive/ronchi_matteo/papers/BMVC15_DescribingCommonVisualActions_PAPER.pdf" target="_blank" onclick="ga('send', 'event', 'Cite', 'Download', 'Paper', 2)">
                  <span class="label_download">PDF</span>
                  </a>
                        
                  <a href="/archive/ronchi_matteo/papers/BMVC15_DescribingCommonVisualActions_SUPP.pdf" target="_blank" onclick="ga('send', 'event', 'Cite', 'Download', 'Supplementary', 3)">
                  <span class="label_download">Supp Mat</span>
                  </a>
                    
                  <a href="/archive/ronchi_matteo/papers/BMVC15_DescribingCommonVisualActions_POSTER.pdf" target="'_blank" onclick="ga('send', 'event', 'Cite', 'Download', 'Poster', 4)">
                  <span class="label_download">Poster</span>
                  </a>
                        
                </li>
              </ul> 
              <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="contact"> </section>
            <h2 style=""><b>Contact</b></h2>
                
                <p>© 2015,
                    <a href="http://vision.caltech.edu/%7Emronchi/">Matteo Ruggero Ronchi</a> and
                    <a href="http://vision.caltech.edu/pietroperona.htm">Pietro Perona</a>
                </p>
                
                <form class="email" role="form" width="100px" id="email-form">
                    <div class="form-group">
                        <label for="name">Your message:</label>
                        <textarea class="form-control" rows="15" id="email-text">Write your email here.</textarea>
                    </div>
                    <div class="form-group">
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(1); return false">Send with email client</button>
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(2); return false">Send with Gmail</button>
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(3); return false">Send with Yahoomail</button>
                    </div>
                </form> 
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>

            <div class="section">
                <a href="http://info.flagcounter.com/J0um">
                <img src="http://s05.flagcounter.com/count/J0um/bg_FFFFFF/txt_000000/border_CCCCCC/columns_3/maxflags_21/viewers_3/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0">
                </a>
            </div>
            
        </div> <!-- /container -->

        <!-- Bootstrap core JavaScript
         ================================================== -->
        <!-- Placed at the end of the document so the pages load faster -->
        <script src="./assets/js/bootstrap.min.js"></script>
        <!-- sendEmail java script source file -->
        <script src="./assets/js/sendEmail.js"></script>
        <script src="./assets/js/authenticatedDownload.js"></script>
        
    </body>
</html>
