---
title: "Perona Lab - Home"
layout: null
excerpt: "Perona Lab"
sitemap: false
permalink: /~mronchi/projects/FaceDistancePortrait
---

<!DOCTYPE html>
<html lang="en">
    
    <head>
        <!-- Page Meta -->
        <meta charset="utf-8">
        <meta http-equiv="Content-Type" content="text/html">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <!-- Content Meta -->
        <title>Distance Estimation of an Unknown Person from a Portrait</title>
        <meta name="description" content="Distance Estimation of an Unknown Person from a Portrait - Project Page">
        <meta name="author" content="Matteo Ruggero Ronchi">
        <meta name="keywords" content="Xavier P. Burgos-Artizzu, Matteo Ruggero Ronchi, Pietro Perona, distance estimation, Faces, camera subject distance, perspective distortion, CMDP, caltech">
        <!-- Icon -->
        <link rel="icon" href="/archive/ronchi_matteo/assets/icon/caltech_transparent.png">
            
        <!-- Bootstrap core CSS -->
        <link href="/archive/ronchi_matteo/assets/css/bootstrap.css" rel="stylesheet">
        <!-- Custom styles for this template -->
        <link href="/archive/ronchi_matteo/assets/theme/navbar.css" rel="stylesheet">
                                        
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
        
    </head>
    
    <body>
        <div class="container">
            
            <div class="page-header">
                <h1>
                    <b>Distance Estimation of an Unknown Person from a Portrait</b>
                    <small>ECCV'14</small>
                </h1>
            </div>
            
            <!-- Static navbar -->
            <div class="navbar navbar-default" role="navigation">
                <div class="container-fluid">
                    
                    <!-- Static navbar HEADER -->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand" href="http://www.caltech.edu"><img src="/archive/ronchi_matteo/assets/icon/caltech_transparent.png" height="50px" /></a>
                    </div>
                    
                    <!-- Static navbar BODY -->
                    <div class="navbar-collapse collapse">
                        <!-- LEFT -->
                        <ul class="nav navbar-nav">
                            
                            <li><a href="#intro">Intro</a></li>
                            <!-- <li class="active"><a href="intro">Intro</a></li> -->
                            <li><a href="#contributions">Contributions</a></li>
                            <li><a href="#results">Results</a></li>
                            <li><a href="#dataset">CMDP Dataset</a></li>
                            <li><a href="#download">Download</a></li>
                            <li><a href="#cite">Cite</a></li>

                        </ul>
                        <!-- RIGHT -->
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="#contact">Contact</a></li>
                            <!--
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Dropdown <span class="caret"></span></a>
                                <ul class="dropdown-menu" role="menu">
                                    <li><a href="#">Action</a></li>
                                    <li><a href="#">Another action</a></li>
                                    <li><a href="#">Something else here</a></li>
                                    <li class="divider"></li>
                                    <li class="dropdown-header">Nav header</li>
                                    <li><a href="#">Separated link</a></li>
                                    <li><a href="#">One more separated link</a></li>
                                </ul>
                            </li>-->
                        </ul>
                    </div><!--/.nav-collapse -->
                    
                </div><!--/.container-fluid -->
            </div><!--/.Static navbar -->
            
            <!-- Main component for a primary marketing message or call to action
            <div class="jumbotron">
                <h1>Navbar example</h1>
                <p>This example is a quick exercise to illustrate how the default, static navbar and fixed to top navbar work. It includes the responsive CSS and HTML, so it also adapts to your viewport and device.</p>
                <p>
                <a class="btn btn-lg btn-primary" href="" role="button">View navbar docs &raquo;</a>
                </p>
            </div> -->
            
            <div class="section">
            <section id="intro"> </section>
            <h2 style=""><b>Introduction</b></h2>
  
                <p align="justify">Consider a standard portrait of a person – either painted or photographed. Can one estimate the distance between the camera (or the eye of the painter) and the face of the sitter? Can one do so accurately even when the camera and the sitter are unknown?
                </p>
  
                <a href="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/facesMontage.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image">
                    <img src="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/facesMontage.png" alt="" height="150">
                </a>
            
                <p align="justify">
                We propose the first automated method for estimating the camera-subject distance from a single frontal picture of an unknown sitter. Camera calibration is not necessary, nor is the reconstruction of a 3D representation of the shape of the subject's head. Our method is divided into two steps: firstly we automatically estimate the location and shape of the subject's face in an image, characterized by 55 costum keypoints positioned on eyes, eyebrows, nose, mouth, head and harline contour. Secondly we train a regressor to estimate the absolute distance from the measurement of changes in the position of these landmarks due to the effect of perspective in images taken at different distances (sometimes informally called "Perspective Distortion").
            
                </p>
                <p align="justify">
                We collected and annotated a dataset of frontal portraits of 53 individuals spanning a number of attributes such as sex, age, ethnicity and hairstyle, each photographed from seven distances - 2, 3, 4, 6, 8, 12 and 16 ft. The proposed method exploits the high correlation between "perspective distortion" and absolute distance and outperforms humans in the tasks of 1) purely estimating the absolute distance and 2) reordering portraits of faces taken at different distances. We observed the phenomenon that different physiognomies will bias systematically the estimate of distance, i.e. some people look closer than others. We explored the importance of individual landmarks in the execution of both task.
            
                </p>
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="contributions"> </section>
            <h2 style=""><b>Contributions</b></h2>
            
                <ul>
                    <li>
                        <p align="justify">
                        A novel approach for estimating the camera-subject distance from a single 2D portrait photograph when both the camera and the sitter are unknown. Our method yields useful signal and outperforms humans on two complex tasks.
                        </p>
                    </li>
                
                    <li>
                        <p align="justify">
                        The introduction of a new dataset of portraits, Caltech Multi-Distance Portraits (<b>CMDP</b>), composed of 53 subjects belonging to both sexes and a variety of ages, ethnic backgrounds and physiognomies. Each subject was photographed from seven different distances and each portrait manually labeled with 55 keypoints distributed across the sitter's head and face. The dataset is publicly available, see the <a href="#dataset">CMDP</a> section.
                        </p>
                    </li>
                
                    <li>
                        <p align="justify">
                        An in-depth analysis and discussion on the feasibility of the proposed approach. We tackled two different variants of the distance estimation task (classification and regression) and explored what are the most important input visual cues. We studied our method's performances when using both automatically estimated and ground-truth human annotated landmarks. Finally, we compared our results with the performances of human observers on the same tasks. Interestingly, we discovered that the main source of error for both humans and our method is the variability of faces' physiognomies.
                        </p>
                    </li>
                </ul>
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="results"> </section>
            <h2 style=""><b>Results</b></h2>
            
                <ul>
                    <li><h3 style=""><b>REORDERING TASK</b></h3></li>
                
                    <div class="cf">
                        <div> <img src="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/classification_results.png" width="350px" /> </div>
                        <div><p align="justify">We show the results obtained by three variants of our method and by humans asked to perform the exact same task of sorting a random permutation of all 7 pictures of a subject based on their conveyed distance. The ground-truth landmarks based variant (<b>MANUAL</b>) outperforms human performance by 16%, while the automatic based ones (<b>RCPR-CMDP</b> and <b>RCPR-300F</b>) are slightly behind by 3% and 25% respectively. Closer faces appear to be much easier to classify than distant ones because of their unusual and disproportioned geometry. This has been confirmed by the human subjects of the study, stating their difficulty in telling apart images in the middle distance-range. Our best variant outperforms human capabilities in the classification task, correctly reordering an average of 81% of the faces when random chance is merely 15%. The same method using machine estimated landmarks still classifies correctly 62% of the images, and could very likely be improved just by increasing the availability of training examples.</p>
                        </div>
                    </div>
                
                    <li><h3 style=""><b>REGRESSION TASK</b></h3></li>
                
                    <div class="cf">
                        <div> <img src="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/regression_results.png" width="350px" /> </div>
                        <div><p align="justify">There is a strong correlation between ground-truth distances and predictions of our method. <b>MANUAL</b> achieves 75% correlation with a coefficient of determination of R2 = .5, while <b>RCPR-CMDP</b> and <b>RCPR-300F</b> achieve 65% and 45% correlation and R2 = .48 and .46 respectively. All variants seem to struggle more with the larger distances, as noticeable from the higher variance and greater distance to ground truth. This is an expected result considering the lower effect of perspective differences between two images taken from afar. As expected, directly estimating the distance of an unknown face proved to be a harder task. Nonetheless, a correlation of 75% with ground-truth indicates that the method is learning well.</p>
                        </div>
                    </div>
                    
                    <li><h3 style=""><b>PHYSIOGNOMY FINDINGS</b></h3></li>
                    
                        <p align="justify">We measured how well each landmark group (head contour, face contour, eyes, nose, mouth) compares to best performance when only that particular group is used. For both <b>MANUAL</b> and <b>RCPR-CMDP</b>, in the classification task, best results are achieved using the head contour and the nose, while the eyes seem to be the least useful.</p>
                    
                        <div> <img src="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/classification_phys.png" width="350px" /> </div>
                        
                        <p align="justify">Also in the regression task, for both <b>MANUAL</b> and <b>RCPR-CMDP</b>, the most discriminative landmark group is the nose.</p>
                        
                        <div> <img src="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/regression_phys.png" width="350px" /> </div>
                        
                        <p align="justify">Discriminative power on these tasks is quantified as how well the method performed when incorporating those input landmarks into the learning. These findings agree with human annotators, which consistently reported during their psychophysics experiments that the use of the deformation in a subject’s nose was the most informative visual cue.</p>

                </ul>
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="dataset"> </section>
            <h2 style=""><b>CMDP Dataset</b></h2>
                
                <p align="justify">We collected a novel dataset, the Caltech Multi-Distance Portraits (<b>CMDP</b>). This collection is made of high quality frontal portraits of 53 individuals against a blue background imaged from seven distances spanning the typical range of distances between photographer and subject: 2, 3, 4, 6, 8, 12, 16 ft (60, 90, 120, 180, 240, 360, 480 cm). For distances exceeding 5m, perspective projection approaches a parallel projection (the depth of a face is about 10cm), therefore no samples beyond 480 cm were needed. Participants were selected among both genders, different ages and a variety of ethnicities, physiognomies, hair and facial hair styles, to make the dataset as heterogeneous and representative as possible.
                </p>
                
                <div class="cf">
                    <div>
                        <img src="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/CMDP_diversity.png" alt="" width="100%">
                    </div>
                    <div>
                        <a href="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/CMDP_montage.png" class="thumbnail" rel="tooltip" data-placement="top" title="Click for large image"> <img src="/archive/ronchi_matteo/projects/FaceDistancePortrait/Figures/CMDP_montage.png" alt="" width="100%">
                        </a>
                    </div>
                </div>
                
                <p align="justify">Pictures were shot with a Canon Rebel Xti DSLR camera mounting a 28-300mm L-series Canon zoom lens. Participants standing in front of a blue background were instructed to remain still and maintain a neutral expression. The photographer used a monopod to support the camera-lens assembly. The monopod was adjusted so that the height of the center of the lens would correspond to the bridge of the nose, between the eyes. Markings on the ground indicated seven distances. After taking each picture, the photographer moved the foot of the monopod to the next marking, adjusted the zoom to fill the frame of the picture with the face, and took the next picture. This procedure resulted in seven pictures (one per distance) being taken within 15-20 seconds. Images were then cropped and resampled to a common format. The lens was calibrated at different zoom settings to verify the amount of barrel distortion, which was found to be very small at all settings, and thus left uncorrected. Lens calibration was then discarded and not used further in our experiments.
                </p>
                <p align="justify">All images in the dataset were manually annotated by three human annotators with 55 facial landmarks distributed over and along the face and head contour. To check consistency of annotations, randomly selected images from different subjects were doubly annotated. Annotators resulted to be very consistent, showing an average disagreement between them smaller than 3% of the interocular distance, and not varying much across distances. The location of the custom keypoints is very different from the positions typically used in literature, more focused towards the center and bottom of the face, as for example Multi-pie format. We purposely wanted to have landmarks around the head contour and all around the face, to sample a larger area of the face.
                </p>
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="download"> </section>
            <h2 style=""><b>Download</b></h2>
                
                <p align="justify">The following buttons will download the <b>CMDP</b> dataset fully annotated and the Matlab code for visualizing the results of the landmark estimation algorithm (<a href="http://www.vision.caltech.edu/xpburgos/ICCV13/">RCPR</a>) on the <b>CMDP</b> dataset and reproducing the paper's results on both the classification and regression tasks. Full details are described in the README file.
                </p>
                <!--<p><b><font color="red">Downloads Temporarily Unavailable, please email me for info.</font></b></p>-->
                
                <div class="row text-center">
                    <a class="btn btn-large btn-primary" type="button" href="/archive/ronchi_matteo/projects/FaceDistancePortrait/Code/DISTANCE-ESTIMATION-DEMO.zip" target="_blank" onclick="_gaq.push(['_trackEvent','Download','CODE',this.href])">
                        <b> Code </b>
                    </a>
                    <a class="btn btn-large btn-primary" type="button" href="https://data.caltech.edu/tindfiles/serve/11d1f564-8dd0-4661-abf1-824718ccc6de/" target="_blank" onclick="_gaq.push(['_trackEvent','Download','CMDP Dataset - Part 1',this.href])">
                        <b>CMDP Dataset - Part 1</b>
                    </a>
                    <a class="btn btn-large btn-primary" type="button" href="https://data.caltech.edu/tindfiles/serve/13a0a000-0977-4cfe-ab5a-ee79096dbb47/" target="_blank" onclick="_gaq.push(['_trackEvent','Download','CMDP Dataset - Part 2',this.href])">
                        <b>CMDP Dataset - Part 2</b>
                    </a>
                    <a class="btn btn-large btn-primary" type="button" href="https://data.caltech.edu/tindfiles/serve/62e34c2e-0fa6-4625-930b-0e07d5bb536d/" target="_blank" onclick="_gaq.push(['_trackEvent','Download','CMDP Dataset Annotations',this.href])">
                        <b>CMDP Annotations</b>
                    </a>
                </div>
                
                Notes:
                <ul>
                    <li>Code Version [1.0.0].</li>
                    <li>Dataset Version [2.0.0]: 51 subjects, 7 distances, 3 annotation types (Manual, RCPR-CMDP, RCPR-300F), pictures in original and standardized version for each annotation type.</li>
                    <li>RCPR-CMDP annotations are available only for the 16 subjects in the test set. Soon we will release them for the full dataset.</li>
                    <li><a href="http://vision.ucsd.edu/%7Epdollar/toolbox/doc/"> Piotr Dollar's Image &amp; Video Matlab Toolbox </a> is a required external library.</li>
                    <li> This code is licensed under the <a href="http://en.wikipedia.org/wiki/BSD_licenses" target="blank">Simplified BSD License</a>.</li>
                </ul>
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="cite"> </section>
            <h2 style=""><b>Cite</b></h2>
                <p>If you find our paper or the released data or code useful to your work, please cite:</p>                
                <pre>@incollection{</br>  perona2014PortraitDistanceEstimation,</br>  title={Distance Estimation of an Unknown Person from a Portrait},</br>  author={Xavier P. Burgos-Artizzu, Matteo Ruggero Ronchi and Pietro Perona},</br>  booktitle={Computer Vision--ECCV 2014},</br>  pages={313--327},</br>  year={2014},</br>  publisher={Springer},</br>  doi={10.1007/978-3-319-10590-1_21}</br>} </pre>
                <ul>
                    <li>
                        Distance Estimation of an Unknown Person from a Portrait <br>
                        
                        X. P. Burgos-Artizzu, M.R. Ronchi and P. Perona <br>
                        
                        ECCV 2014, Zurich, Switzerland, September 2014. <br>
                        
                        <a href="/archive/ronchi_matteo/projects/FaceDistancePortrait/Cite/ECCV14_FacesDistancePortrait_BIBTEX" target="_blank">
                            <span class="label_download">Bibtex</span>
                        </a>
                        <a href="/archive/ronchi_matteo/papers/ECCV14_FaceDistancePortrait_PAPER.pdf" target="_blank" onclick="_gaq.push(['_trackEvent','Download','Paper PDF',this.href])">
                            <span class="label_download">PDF</span>
                        </a>
                        <a href="/archive/ronchi_matteo/papers/ECCV14_FaceDistancePortrait_SUPP.pdf" target="_blank" onclick="_gaq.push(['_trackEvent','Download','Supp Mat PDF',this.href])">
                            <span class="label_download">Supp Mat</span>
                        </a>
                        <a href="/archive/ronchi_matteo/papers/ECCV14_FaceDistancePortrait_POSTER.pdf" target="'_blank" onclick="_gaq.push(['_trackEvent','Download','Poster PDF',this.href])">
                            <span class="label_download">Poster</span>
                        </a>
                        
                    </li>
                </ul>
  
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
            <div class="section">
            <section id="contact"> </section>
            <h2 style=""><b>Contact</b></h2>
                
                <p>© 2014,
                    <a href="http://vision.caltech.edu/%7Expburgos/">Xavier P. Burgos-Artizzu</a>,
                    <a href="http://vision.caltech.edu/%7Emronchi/">Matteo Ruggero Ronchi</a> and
                    <a href="http://vision.caltech.edu/pietroperona.htm">Pietro Perona</a>
                </p>
                
                <form class="email" role="form" width="100px" id="email-form">
                    <div class="form-group">
                        <label for="name">Your message:</label>
                        <textarea class="form-control" rows="15" id="email-text">Write your email here.</textarea>
                    </div>
                    <div class="form-group">
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(1); return false">Send with email client</button>
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(2); return false">Send with Gmail</button>
                        <button class="btn btn-default" id="send-button" value="click" onclick="sendEmail(3); return false">Send with Yahoomail</button>
                    </div>
                </form>
                
                
                <p class="pull-right"><a href="#">Back to top</a></p>
            </div>
            
	    <div class="section">
		<a href="http://info.flagcounter.com/qEYq">
		    <img src="http://s11.flagcounter.com/count/qEYq/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_18/viewers_3/labels_0/pageviews_1/flags_0/" alt="Flag Counter" border="0">
		</a>
	    </div>
    
        </div> <!-- /container -->
        
        <!-- Bootstrap core JavaScript
         ================================================== -->
        <!-- Placed at the end of the document so the pages load faster -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
        <script src="./assets/js/bootstrap.min.js"></script>
        <!-- sendEmail java script source file -->
        <script src="./assets/js/sendEmail.js"></script>
        <script src="./assets/js/sendGmail.js"></script>
        
        <!-- Google Analytics -->
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
             m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
             })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
             
             ga('create', 'UA-54004953-1', 'auto');
             ga('send', 'pageview');
        </script>
        
    </body>
</html>
